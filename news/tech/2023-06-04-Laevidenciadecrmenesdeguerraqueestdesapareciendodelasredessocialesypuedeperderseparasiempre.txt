Pruebas de posibles violaciones a los derechos humanos están en riesgo de perderse al ser eliminadas por las  empresas tecnológicas , según evidenció una investigación de la BBC. Las plataformas sociales están eliminando videos gráficos, a menudo utilizando inteligencia artificial, sin importar que imágenes que pueden ayudar a los enjuiciamientos de los perpetradores de estos abusos puedan ser retiradas sin ser archivadas antes. Meta y YouTube afirmaron que su objetivo es equilibrar sus obligaciones  de informar y de proteger a los usuarios de contenidos nocivos . Sin embargo, Alan Rusbridger, que forma parte del Consejo de Supervisión de Meta, aseveró que la industria está siendo "demasiado cautelosa" en su moderación. Las empresas explicaron que tienen excepciones para el material gráfico cuando es de interés público, pero cuando la BBC intentó subir imágenes que documentaban ataques contra civiles en Ucrania  fueron eliminadas rápidamente . La inteligencia artificial (IA) puede borrar contenidos nocivos e ilegales a gran escala. Sin embargo, cuando se trata de moderar imágenes violentas relacionadas con guerras, los programas son  incapaces de identificar lo que podrían ser violaciones de los derechos humanos . El ejemplo más reciente Ihor Zakharenko, antiguo periodista especializado en turismo, se topó con este problema en Ucrania. Desde la invasión rusa ha estado documentando ataques a civiles. La BBC se reunió con él en un suburbio de Kyiv donde, hace un año, hombres, mujeres y niños murieron por disparos de las tropas rusas cuando intentaban huir de la ocupación. El hombre  filmó los cadáveres -al menos 17- y los vehículos calcinados . Quiso colgar los vídeos en Internet para que el mundo viera lo ocurrido y contrarrestar la versión del Kremlin. Pero  cuando los subió a Facebook e Instagram fueron retiradosrápidamente . " Los propios rusos decían que eran falsos , que no habían tocado a civiles y que sólo habían luchado con el ejército ucraniano", explicó Ihor. La BBC subió las imágenes de Zakharenko a Instagram y YouTube utilizando cuentas falsas. Instagram retiró tres de los cuatro vídeos  en menos de un minuto . Al principio, YouTube aplicó restricciones de edad a los mismos tres, pero  10 minutos después los retiró todos . Lo intentamos de nuevo, pero no se cargaron. La firma rechazó una solicitud para restaurar los vídeos, pese a alegar que incluían  pruebas de crímenes de guerra . Piden afinar los sistemas Figuras clave del sector afirmaron que es urgente que las redes sociales eviten que este tipo de información desaparezca. "Puedes ver que han desarrollado y entrenado a sus máquinas para que, en el momento en que vean algo que parece traumático lo retiren", dijo Rusbridger a la BBC. El Consejo de Supervisión de Meta, del que forma parte, fue creado por Mark Zuckerberg y se conoce como una especie de  "Tribunal Supremo" independiente  dentro de la compañía propietaria de Facebook e Instagram. “Creo que la siguiente pregunta para ellos es cómo desarrollamos la maquinaria, ya sea humana o artificial, para  tomar decisiones más razonables ” , añadió Rusbridger, antiguo redactor en jefe del diario londinense  The Guardian . La directora de la Oficina para la Justicia Penal Global de Estados Unidos, Beth Van Schaak, afirmó que nadie discute el derecho de las tecnológicas a regular los contenidos. “Creo que donde surge la preocupación es cuando la  información desaparece de repente ” , precisó la responsable del despacho que asesora al gobierno estadounidense en materia de crímenes de guerra y de crímenes contra la humanidad. YouTube y Meta afirmaron que, dentro sus excepciones para imágenes gráficas de guerra de interés público, los contenidos que normalmente se retirarían pueden  mantenerse en línea con visualización restringida a adultos . No obstante, el experimento realizado por la BBC con los vídeos de Zakharenko sugirió lo contrario. Meta afirmó que responde  “a las solicitudes legales válidas de los organismos encargados de hacer cumplir la ley de todo el mundo”  y  “seguimos explorando nuevas vías para apoyar los  procesos internacionales de rendición de cuentas  (…) en consonancia con nuestras obligaciones legales y de privacidad” . YouTube, por su parte, declaró que, aunque tiene excepciones para el contenido gráfico de interés público,  la plataforma no es un archivo . "Las organizaciones y activistas de derechos humanos, investigadores, periodistas ciudadanos y otras personas que documenten abusos de los derechos humanos (u otros posibles delitos) deben  observar las mejores prácticas para asegurar y preservar su contenido ", precisó la firma. El caso sirio La BBC también habló con Imad, quien era propietario de una farmacia en Alepo (Siria), hasta que una bomba del gobierno de Bashar al Asad le cayó cerca en 2013. El hombre recordó como la explosión llenó su establecimiento de polvo y humo. Al oír gritos de auxilio, salió al mercado y vio manos, piernas y cadáveres cubiertos de sangre. Los equipos de la televisión local captaron estas escenas . Las imágenes se publicaron en YouTube y Facebook, pero fueron retiradas. En el caos del conflicto, los periodistas sirios dijeron a la BBC que sus propias grabaciones de las imágenes originales también fueron destruidas en bombardeos. Años después, cuando Imad solicitaba asilo en la Unión Europea (UE), le pidieron que presentara documentos que demostraran que estaba en el lugar de los hechos. "Estaba seguro de que mi farmacia había sido grabada. Pero  cuando entré en Internet, me llevaba a un vídeo borrado ", relató. Protegiendo la evidencia Ante este tipo de incidentes, agrupaciones como Mnemonic, una organización de derechos humanos con sede en Berlín (Alemania), tomó cartas en el asunto y comenzó a archivar las imágenes antes de que desaparezcan. Mnemonic desarrolló una herramienta para  descargar y guardar automáticamente pruebas de violaciones de derechos humanos , primero en Siria y ahora en Yemen, Sudán y Ucrania. Hasta ahora han guardado más de  700.000 imágenes de zonas de guerra  antes de que fueran eliminadas de las redes sociales, incluidos tres vídeos que muestran el ataque cerca de la farmacia de Imad. Cada imagen podría contener una pista clave para descubrir lo que realmente ocurrió en el campo de batalla:  el lugar, la fecha o el autor . Pero organizaciones como Mnemonic no pueden cubrir todas las zonas de conflicto del mundo. Demostrar que se han cometido crímenes de guerra es increíblemente difícil, por lo que es vital contar con el mayor número posible de pruebas. " La verificación es como resolver un rompecabezas : se juntan piezas de información aparentemente inconexas para construir una imagen más amplia de lo sucedido", explicó Olga Robinson, de BBC Verify. Institucionalizar la protección de las pruebas La tarea de archivar material de fuentes abiertas -disponible para casi todo el mundo en las redes sociales- recae a menudo en personas que ayudan a sus familiares atrapados en conflictos violentos. Rahwa vive en Estados Unidos y tiene familia en la región etíope de Tigray, asolada por la violencia en los últimos años y donde  las autoridades controlan férreamente el flujo de información . Sin embargo, las redes sociales permiten dejar constancia visual de un conflicto que, de otro modo, permanecería oculto al mundo exterior. “Era nuestro deber” , afirmó Rahwa. “Me pasé horas investigando, así que cuando ves que llegan estos contenidos intentas verificarlos utilizando todas las herramientas de inteligencia, de código abierto que tienes a tu alcance, pero no sabes si tu familia está bien” , comentó. Los defensores de los derechos humanos afirmaron que  urge crear un sistema formal para recopilar y almacenar de forma segura los contenidos eliminados . Esto incluiría la conservación de metadatos para ayudar a verificar el contenido y demostrar que no ha sido manipulado. “Necesitamos crear un mecanismo que permita conservar esa información para posibles ejercicios de rendición de cuentas en el futuro. Las plataformas de medios sociales deberían estar dispuestas a llegar a acuerdos con los mecanismos de rendición de cuentas de todo el mundo” , afirmó Van Schaak.