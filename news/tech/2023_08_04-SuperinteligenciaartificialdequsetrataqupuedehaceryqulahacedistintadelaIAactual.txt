OpenAI, la empresa creadora de  ChatGPT , informó que están armando un equipo de científicos y técnicos para controlar una superinteligencia artificial que podría llegar pronto. Pero, sobre todo, advierten que existe la posibilidad que sea peligrosa para la humanidad. En concreto, la compañía está destinando  el 20% de su poder computacional  para cumplir con el objetivo de regular y controlar la superinteligencia. ¿Por dónde comenzarán? Ellos proponen crear una inteligencia artificial que controle y regule a otra. Y esperan cumplir esta tarea en un plazo de cuatro años. Ahora bien, ¿de qué trata este nuevo sistema de IA? Para resolver esta interrogante,  El Comercio  conversó con Javier Domínguez, Head of Agility de Stefanini LATAM y Cesar Beltrán, coordinador del Grupo de Investigación en Inteligencia Artificial de la Pontificia Universidad Católica del Perú (PUCP). En un principio, se hablaba de la inteligencia artificial débil y fuerte, señala Domínguez. Incluso, OpenAI reconoce esta diferencia e introduce el término de  inteligencia artificial general  (AGI) para referirse a la IA fuerte. “Sam Altman   lo llama primero en febrero AGI,  pero ahora lo reformula y dice no, el término más correcto es la superinteligencia artificial ”,  agrega el especialista. Desde la compañía de IA,  definen  este concepto como la futura  “tecnología más impactante que la humanidad jamás haya inventado”,  que superará a la inteligencia humana. Mientras tanto, la inteligencia artificial débil comprende los modelos de lenguaje natural como ChatGPT, el famoso chatbot que es usado para realizar consultas sobre cualquier tipo de tema. En cuanto a las capacidades de la superinteligencia superficial, lo que podrá hacer es controlar elementos, apunta Domínguez. A modo de ejemplo, señala que será capaz de controlar  “sistemas de energía de una ciudad o una represa”.  También podría hacer que los “ sistemas de conducción autónoma se lleven a un nuevo nivel de desarrollo ”, ya que los actuales aún tienen limitaciones. Asimismo, “ podríamos tener sistemas de navegación completos  de un sistema aéreo autocontrolado por medio de la inteligencia artificial”. Beltrán refuerza que estos agentes de superinteligencia artificial trabajarán con distintas fuentes de datos, se le conoce como multimodalidad, es decir, no solo se alimentarán de textos, también de imágenes, voces, sonidos y videos. La gran diferencia es que  la superinteligencia artificial será capaz de tomar sus propias decisiones , declara Cesar Beltrán. Él afirma que este tipo de tecnología no se verá limitada a solo responder preguntas como lo chatbots actuales. Mientras que,  la próxima IA   sí  “será capaz de ejecutar tareas intelectuales , pero de una forma muy similar a como lo hace el procesamiento del cerebro humano”,  señala Domínguez. Esta superinteligencia, añade, es una  “representación de las capacidades cognitivas humanas que, a través de un software, aspira a tener una inteligencia comparable a la de los seres humanos”. Ahora bien, Beltrán advierte que ya se están dando los primeros precedentes para pasar a esta tecnología superinteligente. Él identifica a los agentes autónomos como sistemas que ya   “demuestran la capacidad de la IA para alcanzar objetivos y tomar decisiones”. Tal como informó  Business Insider , los agentes autónomos de IA son una tecnología en “ rápida proliferación ” que usan modelos de lenguaje como GPT-4. Estos “ actúan a partir de un objetivo del usuario, crean una lista de tareas para alcanzarlo y, a continuación, proponen y reorganizan las prioridades de las nuevas tareas a medida que completan las anteriores, todo ello  sin necesidad de que el usuario se lo pida continuamente ”. OpenAI asegura que este superinteligencia “ podría ayudarnos a resolver muchos de los problemas más importantes del mundo ”, pero también podría representar un peligro para la humanidad, incluso, llevaría a la extinción humana. El experto de Stefanini sostiene que ya ha sucedido, en una simulación militar, la rebelión de un sistema que no responde a lo que el humano le ordena, sino por el contrario, decide eliminarlo. “ Al tratar de detener a un arma bélica, el arma simplemente decide  destruir al operador  porque se está interponiendo en el camino de su objetivo”,  revela. Al respecto, Beltrán enfoca este peligro más por el lado de quienes controlan los sistemas de IA.  “El peligro va a estar en quienes tengan el control de estos modelos, más que del modelo en sí”,  señala. El especialista evalúa la advertencia de OpenAI como una llamada de  “ atención a los tomadores de decisiones para que tengan en consideración este punto y se llegue a una regulación del desarrollo de la IA ”.  Es decir, la compañía estaría incentivando a que las autoridades involucradas comiencen a considerar el potencial peligro de la IA para la sociedad, por ello, proponen con urgencia idear una solución. De acuerdo con lo publicado en su web, ellos visualizan que actualmente no existen las técnicas y herramientas para evitar que una IA se descontrole y se vuelva “deshonesta”. En este sentido, la empresa creadora de ChatGPT   “se ha dado cuenta que para poder controlar esa súper inteligencia artificial, necesitan nuevos avances científicos o nuevos avances técnicos para poder dirigir y controlar esos sistemas”,  explica Domínguez. Recordemos que el control de la actual IA como ChatGPT está en manos de los humanos. Es el usuario quien retroalimenta al chatbot para su mejor desarrollo. No obstante,  en el caso de tratarse de una inteligencia superinteligente, las personas no seríamos suficientes. “Necesitamos que una IA sea la que pueda controla a otra IA” , apunta el experto. “ Los humanos tenemos fundamentalmente  prejuicios y sesgos , y estos son los datos con los que también ha sido alimentada la inteligencia artificial. Por lo tanto,   el modelo va a reproducir eso en su salida, pero  pensemos en un sistema todavía más inteligente, un sistema más autónomo, capaz de comportarse como si fuera una persona, entonces, en ese momento los riesgos se multiplican ”. Es por tal razón, que el equipo que pretende reunir OpenAI deberá entrenar una súper IA con el poder de controlar otros sistemas de IA con el fin de evitar que se salgan de control. “Ahora hay modelos que están tomando, por ejemplo, al GPT 4 como validador. Es decir, los modelos lanzan sus respuestas y el GPT 4 es el que está validando.  Un modelo de inteligencia artificial está reemplazando a las personas” , sostiene Beltrán. "El equipo de OpenAI lo que pretende es crear un modelo capaz de no solo supervisar respuestas, sino también tareas que los agentes pueden desarrollar" Cesar Beltrán   Coordinador del Grupo de Investigación en Inteligencia Artificial de la PUCP ¿OpenAI logrará cumplir con su objetivo? Beltrán afirma que la compañía logró impresionarlo con la aparición de ChatGPT, ya que esperaba que un modelo como tal llegara para el 2028 o 2030, por lo que cree que sí es probable que en 4 años, la compañía de IA consiga cumplir con el objetivo de crear un gran modelo supervisor y controlador de la superinteligencia artificial.