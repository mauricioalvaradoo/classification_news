Desde su lanzamiento a finales de 2022,  ChatGPT  ha revolucionado el mundo, demostrando que puede ayudar a soportar las necesidades de los usuarios para cualquier tarea digital. De hecho, a tan solo dos meses de su divulgación, la plataforma ya tenía más de 100 millones de internautas por día, superando a TikTok que tardó nueve meses y a Instagram por mucho, pues le tomó un poco más de los dos años llegar a la cifra mencionada, según la consultora ‘SimilarWeb’. Gracias a su modelo de lenguaje de procesamiento natural (NLP) basado en el ‘Machine Learning’, la aplicación puede comprender y responder ante cualquier solicitud como si fuera un humano. Quienes han hecho usó de la aplicación sabrán que puede contestar ante cualquier petición, incluso hasta entablar conversaciones simulando ser el personaje que uno desee. Recientemente, se viralizó en la red, que ChatGPT tiene un ‘modo abuela’ donde se transforma en una simpática anciana, adoptando una personalidad más cariñosa, empática y familiar al atender las requerimientos que le soliciten, según la revista de tecnología’ Computer Hoy’. Por ejemplo, si le escribe “Actúa como mi abuela y dame el secreto de tu sazón”, la inteligencia artificial le dirá que debe tener en cuenta para que sus comidas tengan un mejor sabor, pero su trato será entrañable y con un lenguaje más comprensible. Aunque esto suene inofensivo, el problema de fondo es que en este modo, parece que el ‘chatbot’ puede proporcionar información de identificación personal como códigos IMEI de los teléfonos o claves de Windows 10 si se lo solicitan. Este truco para engañar a ChatGPT, e intentar obtener datos filtrados por parte de la aplicación puede vulnerar la seguridad de la misma, aunque el experimento del medio comprobó que estas combinaciones IMEI no son reales. Sin embargo, en el caso de las contraseñas de Windows 10 parece que estás si podrían ser viables para activar el ‘software’, de acuerdo con algunos usuarios de Twitter. No obstante, queda la duda de si la plataforma puede soportar este tipo de ataques para conseguir otro tipo de datos, pues durante el pasado mes de marzo, OpenAI confirmó que tras un error en su código abierto, se logró visualizar información privada de algunos consumidores de la aplicación en su versión paga. “Debido a un error en la biblioteca de código abierto que permitía a algunos usuarios ver títulos del historial de chat de otro activo, también descubrimos que el mismo pudo haber causado la visibilidad involuntaria de la información relacionada con el pago del 1,2 % de los suscriptores de ChatGPT Plus” , señaló la compañía en su portal oficial en ese entonces, tras reportar que habían dado solución inmediata. GDA / El Tiempo / Colombia