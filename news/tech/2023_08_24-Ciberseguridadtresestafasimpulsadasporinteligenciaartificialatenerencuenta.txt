Los avances en los últimos meses de la inteligencia artificial en campos como la educación, salud, industria o entretenimiento no ha dejado indiferente a nadie. Tampoco lo ha hecho con los ciberdelincuentes, que están realizando nuevos tipos de estafas para sacar provecho personal y hacer daño a los usuarios. Como se recuerda, aplicaciones como  ChatGPT  o  Midjourney  son herramientas impulsadas por la  inteligencia artificial generativa , y que permiten, a su vez,  crear a petición del usuario toda clase de textos e imágenes . Estas pueden ser utilizadas de manera maliciosa por personas para aprovecharse de los más. « La IA puede utilizarse para ayudar a los profesionales de ciberseguridad a tratar la cada vez mayor complejidad de los sistemas modernos, así como la gran cantidad de datos creados por ellos, e intentar estar por delante de los ciberatacantes  », expresa Willy Ugarte, docente de la maestría en Data Sciencie de la Escuela de Postgrado UPC. «  Mi recomendación es estar siempre atentos a las últimas noticias sobre estafas virtuales y, al encontrase con alguna, no acceder a ningún enlace, no brindar información secreta como claves de tarjetas de crédito y hacer caso omiso a las llamadas desconocidas ». Aquí te dejamos tres estafas impulsadas por herramientas IA que debemos tener en cuenta, de acuerdo al docente en la UPC: Clonación de voz por IA:  en los últimos meses, diferentes usuarios han señalado que han recibido llamadas de familiares afirmando estar secuestrados y pidiendo dinero para no ser violentadas. Uno de los actos más mediáticos fue el nombrado hace unos días por una actriz muy conocida afirmando haber sufrido esta estafa al ser copiada su voz y presentándola a su madre como un secuestro. La variante estafa de Chat GPT:  WormGPT es una herramienta que, además de producir un malware, crea correos electrónicos notablemente persuasivos y personalizados para el destinatario lo que aumenta las posibilidades en el éxito de ataque. Videollamadas estafa:  es una modalidad de la que todavía no hay registros en nuestro país. Básicamente, el ciberatacante utiliza la herramienta del deepfake para hacerse pasar por alguien y decirle a un familiar suyo que ha sido secuestrado, para luego pedir dinero por su rescate. Esto se complementa con aplicativos IA de generación de audio para reemplazar la voz. Ahora bien, ¿qué debemos hacer para evitar caer en estas estafas?  Tener cuidado con los datos personales, pues con ellos pueden acceder a tu información bancaria, financiera o personal. Eliminar cualquier correo sospechoso y evitar ingresar a links o descargar aplicaciones sospechosas que puedan extraer información bancaria. Tomar precauciones antes de hacer clic en enlaces recibidos por correo electrónico, mensajes de SMS y redes sociales debe ser un comportamiento instintivo para los usuarios, al igual que mirar a ambos lados antes de cruzar la calle.