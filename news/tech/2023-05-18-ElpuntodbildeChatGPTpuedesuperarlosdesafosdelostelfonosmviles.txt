OpenAI utiliza cientos de GPUs profesionales costosas para brindar el servicio de  ChatGPT , lo que resulta en un alto costo para adaptarlo al formato de los smartphones. En la competencia por los modelos de  IA  generativa, surgen nuevos desafíos: permitirnos utilizar estos modelos no solo en la nube, sino también localmente en nuestros teléfonos, incluso sin conexión a internet. Aunque parezca inimaginable debido a los grandes recursos que requiere ChatGPT, en las últimas semanas han surgido varios proyectos que se dirigen al objetivo de  crear un miniChatGPT para dispositivos móviles.  Un ejemplo de lo que se busca podría ser Gecko, una variante propuesta por Google para implementar su nuevo modelo LLM PaLM 2, que compite con el GPT-4 de OpenAI. Según Google, Gecko es lo suficientemente pequeño como para ejecutarse directamente en smartphones. Han logrado hacerlo funcionar en un Samsung Galaxy, aunque aún no han demostrado públicamente esta capacidad. Por otro lado, empresas como Qualcomm están explorando plataformas híbridas de inteligencia artificial que combinan modelos como ChatGPT en la nube con otros como Gecko en el móvil. Cristiano Amon, CEO de Qualcomm, ha destacado que depender únicamente de modelos en la nube sería costoso.  La combinación de modelos LLM capaces de ejecutarse en dispositivos móviles permitiría reducir los costos.  Qualcomm ya ha experimentado con esta opción y ha logrado que Stable Diffusion se ejecute de forma nativa y local en uno de sus SoC. Cada vez aparecen más proyectos de modelos de IA generativa preparados para funcionar en dispositivos móviles. Un ejemplo es el proyecto de código abierto MLC LLM, cuyo objetivo es implementar modelos LLM en diversas plataformas de hardware, incluyendo dispositivos móviles como MacBook, iPad o iPhone 14 Pro. Aunque su rendimiento es modesto, aproximadamente 7,2 tokens/segundo en un iPhone 14 Pro, se espera un avance continuo en esta área.