Las herramientas y funciones de accesibilidad de Apple incluirán con  iOS  17 novedades para las personas que no pueden hablar o están en riesgo de perder el habla, quienes podrán usar un dispositivo iPhone o iPad para expresarse con Live Speech y mediante la creación de una voz personalizada. La nueva herramienta  ‘Live Speech’  se dirige a las personas que no pueden hablar para que puedan comunicarse durante las llamadas y videollamadas con  ayuda del teclado del iPhone o del iPad . Para ello, podrán escribir lo que quieran decir y este texto se reproducirá en voz alta posteriormente, para mantener la conversación. ‘Live Speech’ también  permitirá guardar frases que el usuario utilice de forma habitual para responder más rápido , como informa la compañía tecnológica en su blog oficial. También puede utilizarse en conversaciones en persona. Junto a esta herramienta, Apple también ha anunciado  ‘Personal Voice’ , con la que aquellas personas que estén perdiendo la capacidad de hablar puedan recrear su voz a partir de un entrenamiento breve, para que funcione de forma conjunta con ‘Live Speech’. Para ello, deberán leer conjunto al azar de propuesta de texto para  grabar 15 minutos de audio con su voz en iPhone o iPad.  A partir de esta información el modelo de aprendizaje automático del dispositivo recreará su voz en las llamadas. Las novedades en accesibilidad se extienden a las herramientas para las personas con discapacidades cognitivas. Para ellas, Apple ha anunciado  ‘Assistive Access’,  que personaliza algunas de las ‘apps’ de la compañía para ofrecer una interfaz de usuario simplificada, con iconos y textos grandes. La finalidad de ‘Assistive Access’ es  personalizar la experiencia de iPhone y FaceTime , y para ello incluye opciones de configuración para los colaboradores de confianza. También se puede optar por un diseño más visual basado en cuadrículas para la pantalla de inicio y las aplicaciones, u otro en filas, en el que se destaca el texto. Los usuarios con discapacidades visuales también tendrán una novedad en la lupa, una nueva función  ‘Apuntar y hablar’, que utiliza el escáner LiDAR, la cámara y el aprendizaje automático del dispositivo  para explicarle al usuario las etiquetas de texto que muestra el objeto que tiene delante y pueda interactuar con él. Por ejemplo, el usuario puede  apuntar con la cámara del móvil a un microondas , y una voz le indicará los distintos botones que tiene el electrodoméstico, para que pueda utilizarlo. Apple ha matizado que estará disponible en inglés, francés, italiano, alemán, español, portugués, chino, cantonés, coreano, japonés y ucraniano. Aunque esta función de ‘apuntar y hablar’ se encuentra dentro de la Lupa, también es compatible con el lector ‘VoiceOver’ y con otras funciones presentes en Lupa, como la detección de personas, la detección de puertas y la descripciones de imágenes. Todas estas herramientas de accesibilidad están disponibles en una vista previa, y llegarán a finales de año, junto con la personalización de velocidad de Siri en  ‘VoiceOver’  o la posibilidad de emparejar dispositivos auditivos Made for iPhone directamente con Mac y personalizarlos para la comodidad de los usuarios sordos o con problemas de audición.