OpenAI  está reuniendo un equipo de especialistas en aprendizaje automático para controlar sistemas de  IA  más inteligentes que los humanos. Estos modelos, asegura la empresa creadora de ChatGPT, podría ser un peligro hasta el punto de llegar a la extinción humana. La compañía denomina a estos sistemas como una ‘superinteligencia’ “ que podría llegar en esta década”, e xplicó OpenAI en su  blog . Asimismo, reconoce que actualmente, no cuentan con “ una solución para dirigir o controlar una IA potencialmente superinteligente y evitar que se vuelva deshonesta”. En base a ello,   OpenAI está creando un nuevo equipo de investigadores e ingenieros que trabajarán en la problemática. En concreto, están dedicando  el 20% de su poder en computación  para cumplir con su objetivo que está previsto para resolverse  dentro de 4 años.  Por ahora, el equipo está liderado por Ilya Sutskever, cofundador y científico jefe de OpenAI, junto a Jan Leike, líder del equipo de alineación. “ Estamos buscando nuevos investigadores e ingenieros destacados para que se unan a este esfuerzo.  La alineación de superinteligencia es fundamentalmente un problema de aprendizaje automático y creemos que los grandes expertos en aprendizaje automático, incluso si aún no están trabajando en la alineación, serán fundamentales para resolverlo”,  dijo en su blog la compañía. Lo que hará este nuevo equipo es  crear una inteligencia artificial de nivel casi humano  que regulará la superinteligencia artificial. Es decir, entrenarán los sistemas actuales para que evalúen otros sistemas de IA con el fin de evitar que se salgan de control. “ Para alinear al primer investigador de alineamiento automatizado, necesitaremos:  desarrollar un método  de entrenamiento escalable,  validar el modelo  resultante y  hacer una prueba  de esfuerzo de toda nuestra canalización de alineamiento ”, informó la compañía.