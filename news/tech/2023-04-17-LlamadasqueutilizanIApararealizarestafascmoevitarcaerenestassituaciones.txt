Cada vez se registran más casos de usuarios que reciben supuestas llamadas telefónicas de un ser querido o un amigo solicitando su ayuda y dinero para salir de una situación urgente, y ante la incertidumbre, lo primero en lo que se piensan es en ayudar en lugar de que están siendo víctimas de una estafa que utiliza la  inteligencia artificial  (IA) para imitar voces de allegados y conseguir dinero. Las estafas telefónicas son un  método muy recurrente para los actores maliciosos  a la hora de intentar timar a los usuarios, ya sea para conseguir dinero o para robar datos relevantes como contraseñas o claves bancarias. Las técnicas para estas estafas telefónicas cada vez evolucionan más intentando esquivar las barreras de protección que se van interponiendo entre los malhechores y sus objetivos. Sobre todo, añadiendo a su modus operandi las funciones más nuevas que brinda la tecnología como, por ejemplo, la  IA generativa. Los  estafadores están utilizando programas de generación de voz con tecnología de Inteligencia Artificial  para, a través de llamadas telefónicas, hacerse pasar por personas cercanas a las víctimas e intentar conseguir dinero. En la llamada,  los actores maliciosos, que se hacen pasar por un ser querido, cuentan que se encuentran en una situación de peligro y piden ayuda y dinero urgente a las víctimas. Normalmente, los estafadores se escudan en  imitar a familiares o personas muy cercanas a la víctima , como hijos o hermanos, para que haya menos resistencia a dar dinero o se dude menos sobre la legitimidad estas llamadas. Así, aunque durante la llamada la víctima se encuentre extrañada y haya detalles que no logre entender de la historia, este tipo de estafadores  utilizan el elemento definitivo para convencer a la víctima de que se trata de una persona cercana y debe ayudar: la voz.  Para ello, los timadores utilizan programas generadores de voz basados en IA. Según datos de la Comisión Federal de Comercio (FTC) recogidos por The Washington Post,  las estafas telefónicas causaron la pérdida de hasta 11 millones de dólares durante el año 2022 , convirtiendo las estafas de impostores en el segundo tipo más común en Estados Unidos. Estos programas de IA analizan la voz de la persona a la que se quiere imitar y busca los patrones que consolidan los matices y el sonido único de la persona en cuestión al hablar. Es decir,  los programas se entrenan para imitar el tono, el acento e, incluso, la edad, y luego recrearlos. Además, estos programas son capaces de aprender a imitar voces en cuestión de segundos. Para ello, solo necesitan como base una pequeña muestra de audio. Por ejemplo,  en algunos programas basta con utilizar 30 segundos de la persona hablando para poder imitarla. Estos audios se pueden obtener de  cualquier video en el que salga hablando la persona a la que se quiere imitar , por ejemplo, en las publicaciones personales en redes sociales como  Instagram o TikTok , o incluso, en videos de YouTube o podcast. El aumento de este método de estafa puede basarse en que, además de su efectividad, la imitación de voces es algo sencillo y barato. No obstante, hay otras empresas que también desarrollan este tipo de tecnología como son Murf.ai; Play.ht; y Respeecher, cuyo precio varía entre los 20 y 30 dólares mensuales. Por ejemplo, en el caso de Respeecher, incluye una función para convertir la voz en tiempo real. Esto es, a la vez que el usuario habla, la IA cambia el tono a la voz imitada instantáneamente. Antes de que existiesen estos programas avanzados de imitación de voz, los usuarios podían fijarse en algunas señales para identificar si una llamada estaba siendo realizada por un programa de generación de voz por inteligencia artificial. Por ejemplo, según señala el analista en ciberseguridad de Kaspersky, Marc Rivero,  una de estas señales era el lenguaje forzado o “robotizado” de la voz que utilizaba el programa.  Otra señal podía ser una pausa breve tras una intervención del usuario, ya que el sistema debía procesar la información. Según Rivero, también se podía identificar la “falta de interacción humana típica”, como la capacidad de responder a preguntas imprevistas. De cara a estas nuevas estafas, en las que sí se utilizan programas de IA que imitan la voz deseada, el director técnico de Check Point Software para España y Portugal, Eusebio Nieva, sugiere tomar algunas precauciones. Aunque reconoce que cada vez es más complicado realizar verificaciones  “debido al grado de especialización y competencia de estos programas de IA” ,  se debe asumir  “una filosofía de desconfianza”  con respecto a algunas llamadas , sobre todo, si vienen de un origen desconocido e implican transacciones monetarias,  “aunque la voz sea identificable” . En este sentido, tal y como explica Eusebio Nieva, en caso de recibir una llamada de estas características, en cuanto el usuario identifique una mínima sospecha, debe  “establecer algún tipo de identificación para determinar sin ningún tipo de duda la identidad del hablante del otro extremo”. Algunas opciones planteadas por Nieva son  intentar hablar de algún tema que solamente la persona real supiese o plantear “pequeñas trampas”  durante la conversación que indiquen que hay algún tipo de intento de engaño. Siguiendo este hilo, otra forma de asegurar estas llamadas es  “establecer un protocolo de doble autenticación del llamante para evitarlo” . Por ejemplo,  solicitar una llamada de vuelta al comunicante para determinar que realmente se trata de un ser conocido. Estas sugerencias también son aplicables a entornos laborales, sobre todo,  “sabiendo que el objeto del timo será algún administrativo o directivo del departamento financiero” , señala Nieva.