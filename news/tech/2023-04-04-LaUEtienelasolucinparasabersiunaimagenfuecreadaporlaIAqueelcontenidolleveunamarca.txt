La Unión Europea está buscando que el contenido generado por una  inteligencia artificial  tenga alguna indicación para que el usuario sepa que ha sido generado por este sistema. Por ello, quiere establecer que estas imágenes, textos o audios especifiquen a través de una marca o etiqueta que su autor ha sido una IA. “ Una advertencia de ‘Made by AI’ . Para que no haya confusión posible, la UE quiere establecer,  de manera obligatoria, que casi todo el contenido generado por IA esté advertido por una etiqueta ”, señala  Xataka . El objetivo es que cualquier persona pueda diferenciar entre lo real y lo creado por una IA. “ En todo lo que sea generado por inteligencias artificiales, ya sean textos o imágenes,  habrá una obligación de notificar que ha sido creado por una inteligencia artificial ”, asegura el comisario de Mercado Interior, Thierry Breton, según el medio. “ Se debería informar a las personas de que están tratando con un chatbot y no con un ser humano . La transparencia también es importante con respecto al riesgo de sesgo e información falsa ”, agrega. Si bien este tipo de regulaciones vienen siendo impulsadas desde 2021,  con la llegada de inteligencias artificiales como DALL-E, Midjourney y ChatGPT, es más difícil saber si lo que uno ve o escucha ha sido creado por una IA o no . Por ello, la UE está buscando la manera en crear leyes que abarquen cualquier escenario. Cada sistema de inteligencia artificial debe mostrarle al usuario que lo que está leyendo, viendo o escuchando ha sido generado por una IA. “ Concretamente, estas aplicaciones deberán notificar a los usuarios que están usando e interactuando con un sistema de IA , salvo que sea algo muy evidente, y también si están usando sistemas de reconocimiento emocional o categorización biométrica. También,  todos los deepfakes que imitan la voz o apariencia de una persona deberán llevar una etiqueta o marca de agua advirtiendo lo que son ”, añade el medio. Sin embargo,  la UE tiene cuatro tipos de riesgos para la regulación de la IA, por lo que la marca o etiqueta no irá en todo tipo de sistema con inteligencia artificial . El primer riesgo es el que se considera “inaceptable”, cuyo ejemplo más claro es el que se utiliza en China para puntuar socialmente (social scoring) la credibilidad o reputación de un individuo. El segundo, denominado como “alto riesgo”, son las que utilizan documentos o requisitos legales, como por ejemplo las solicitudes médicas o los sistemas que evalúan los currículums de solicitantes de empleo. En tercer lugar se encuentra  la “IA con obligaciones específicas de transparencia”, en el cual está ChatGPT, Bard, Midjourney, entre otros, y que sí necesitaría esta etiqueta . Finalmente, están los de riesgo “mínimo o inexistente”. Estas inteligencias artificiales no tienen restricciones, pues no manejan datos o sistemas que podrían afectar a las personas.