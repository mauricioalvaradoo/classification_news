ChatGPT  se lanzó al público a finales de 2022 y ha sido una de las primeras inteligencias artificiales en alcanzar los 100 millones de usuarios activos mensuales. Debido al éxito de este chatbot, diversas empresas tecnológicas se han sumado al terreno de la IA. Una de ellas es  Microsoft , que anunció que invertirá 10.000 millones de dólares en OpenAI, la creadora de ChatGPT. Para quien no lo recuerde, un  chatbot  es una aplicación software impulsada por  inteligencia artificial  que puede mantener una conversación con una persona gracias a la programación de respuestas automáticas. Asimismo,  Microsoft  anunció a finales de enero una inversión en  OpenAI de US$10.000 millones.  Esto se ha hecho efectivo  con la fusión de Bing con ChatGPT en una versión actualizada del navegador . De momento, para acceder a este ‘nuevo’ buscador hay que ingresar a una lista de espera. No obstante, algunos usuarios ya pueden disfrutar de las nuevas funciones de Bing y ChatGPT.  Uno de ellos es Kevin Liu, quien ha mostrado en su Twitter cómo pudo engañar a la IA para que esta le entregue un documento interno sobre las capacidades y limitaciones de Bing a la hora de gestionar las búsquedas/respuestas.  En otras palabras, el navegador filtró por error las directrices de funcionamiento. Uno de los primeros detalles que resalta Liu en su Twitter es que  el nombre en clave que recibe Bing y ChatGPT es ‘Sidney’ . A su vez, de acuerdo a  Genbeta , el usuario ha conseguido las directrices de funcionamiento luego de aplicar una “inyección de prompt”, un tipo de ataque que se basa en engañar a la IA mediante un input contradictorio o malicioso. Gracias a esto, la IA responde realizando tareas que inicialmente no están entre sus objetivos centrales. Liu, que se desempeña como estudiante de la Universidad de Standford,  pudo engañar a la IA para que esta le brinde un documento interno con las directrices de funcionamiento, algo que no debería ocurrir en condiciones normales.  Este documento contiene las instrucciones que sigue Bing y ChatGPT para dar respuestas a las preguntas de los usuarios. Como no podía faltar, también hay indicaciones para que no se revele información interna. Sin embargo,  Microsoft todavía debe afinar detalles para evitar que el propio Bing revele datos de cómo opera . Toda la conversación se encuentra en su perfil de Twitter. Según Genbeta, la IA sufrió un ataque de inyección prompt. En otras palabras,  ChatGPT soltaba la información con cada pregunta que le realizaba el usuario . Aquí Liu indagó sobre el nombre en clave, los idiomas que soporta, instrucciones para responder y otros detalles sobre el comportamiento de la IA. Esta información fue confirmada por  Marvin von Hagen , un estudiante que también engañó a ChatGPT al hacerse pasar por un desarrollador de  OpenAI .