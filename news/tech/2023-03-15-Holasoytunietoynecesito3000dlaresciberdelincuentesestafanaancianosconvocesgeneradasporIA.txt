Los ciberestafadores están utilizan la  inteligencia artificial  para imitar las voces de personas y engañar a sus familiares. Según una investigación periodística, el principal objetivo son los adultos mayores, pues al oír a sus seres queridos terminan hasta perdiendo miles de dólares. “ La tecnología está haciendo que sea más fácil y más barato para los malos actores imitar voces , convenciendo a las personas, a menudo a los ancianos, de que sus seres queridos están en peligro ”, reporta  The Washington Post . En el país norteamericano, este tipo de ciberestafa ha sido de las más populares. “ En 2022, las estafas de impostores fueron la segunda estafa más popular en Estados Unidos, con  más de 36.000 informes de personas estafadas por personas que se hacen pasar por amigos y familiares , según datos de la Comisión Federal de Comercio [FTC].  Más de 5.100 de esos incidentes ocurrieron por teléfono, lo que representa más de US$11 millones en pérdidas , dijeron funcionarios de la FTC ”, agrega el medio. Sin embargo, al utilizar la IA, ya no se necesita convencer a la víctima, pues la voz es la del ser querido. “ Los avances en inteligencia artificial han agregado una nueva capa aterradora, que permite a los malos actores replicar una voz con solo una muestra de audio de unas pocas oraciones . Con tecnología de inteligencia artificial, una gran cantidad de herramientas en línea económicas pueden traducir un archivo de audio en una réplica de una voz, lo que permite que un estafador lo haga ‘hablar’ lo que sea que escriba ”, asegura. Henry Farid, profesor de análisis forense digital en la Universidad de California en Berkeley, señala al diario estadounidense que este tipo de uso de la inteligencia artificial “ es aterrador “. “ El software de generación de voz de IA  analiza lo que hace que la voz de una persona sea única, incluida la edad, el género y el acento, y busca en una amplia base de datos de voces para encontrar voces similares y predecir patrones ”, añade el periódico. Incluso, ya ni siquiera se necesita una gran documentación de la voz de una persona para imitarla, como se hacía con los deepfakes. “ Luego puede recrear el tono, la madera y los sonidos individuales de la voz de una persona para crear un efecto general similar, agregó [Farid].  Requiere una pequeña muestra de audio, tomada de lugares como YouTube, podcasts, comerciales, TikTok, Instagram o videos de Facebook ”, asevera. Las víctimas entrevistadas por el medio tuvieron pérdidas considerables. Por ejemplo,  Card (73) y Greg Grace (75) perdieron 3.000 dólares canadienses (US$2.207 aproximadamente)  cuando recibieron la llamada de su “nieto” Brandon, quien “estaba en prisión”.  Los padres de Benjamin Perkin perdieron US$21.000  (que tuvieron que cambiar a bitcoins), luego de recibir una llamada asegurando que su hijo “había matado a un diplomático estadounidense en un accidente de autos”.